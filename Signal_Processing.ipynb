{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from numpy.random import seed\n",
    "seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "data = pd.read_csv('table3.csv')\n",
    "data.insert(loc = 1, column = 'F_Returns', value = data['M_Returns'].shift(-1))\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training, validation and test set\n",
    "TrainingSet = data.iloc[0: round(0.4*data.shape[0]), :]\n",
    "\n",
    "ValidationSet = data.iloc[round(0.4*data.shape[0]): round(0.8*data.shape[0]), :]\n",
    "\n",
    "OOSTraining = data.iloc[0: round(0.8*data.shape[0]), :]\n",
    "\n",
    "TestSet = data.iloc[round(0.8*data.shape[0]):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into Training, validation and test set\n",
    "X_train = TrainingSet.drop(['F_Returns'], axis=1).values\n",
    "y_train = TrainingSet['F_Returns'].values\n",
    "\n",
    "X_validation = ValidationSet.drop(['F_Returns'], axis=1).values\n",
    "y_validation = ValidationSet['F_Returns'].values\n",
    "\n",
    "X_oostrain = OOSTraining.drop(['F_Returns'], axis=1).values\n",
    "y_oostrain = OOSTraining['F_Returns'].values\n",
    "\n",
    "X_test = TestSet.drop(['F_Returns'], axis=1).values\n",
    "y_test = TestSet['F_Returns'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "sc_xtr = StandardScaler()\n",
    "sc_xv = StandardScaler()\n",
    "sc_xoostr = StandardScaler()\n",
    "sc_xte = StandardScaler()\n",
    "X_train = sc_xtr.fit_transform(X_train)\n",
    "X_validation = sc_xv.fit_transform(X_validation)\n",
    "X_oostrain = sc_xoostr.fit_transform(X_oostrain)\n",
    "X_test = sc_xte.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.040893, -0.001664,  0.091667, -0.015573, -0.053125, -0.049505,\n",
       "        0.043056,  0.15436 , -0.10465 , -0.032468,  0.10201 , -0.018405,\n",
       "        0.0225  ,  0.089506,  0.10482 , -0.086667, -0.070822,  0.057927,\n",
       "       -0.091643,  0.012821, -0.012658, -0.15    , -0.007634,  0.1     ,\n",
       "        0.007692,  0.17193 ,  0.065868, -0.10618 ,  0.079365,  0.1     ,\n",
       "       -0.039572,  0.067416,  0.11053 ,  0.069194, -0.089286, -0.071078,\n",
       "        0.042744,  0.015306,  0.25126 ,  0.00241 , -0.064516, -0.092672,\n",
       "        0.055107,  0.088435,  0.008333,  0.064463, -0.09375 ,  0.017241,\n",
       "       -0.061017,  0.004545,  0.076923, -0.07521 ,  0.006865, -0.054545,\n",
       "        0.094231, -0.042035,  0.11316 ,  0.017012, -0.034908, -0.080851,\n",
       "        0.049074,  0.033333, -0.10538 , -0.059615,  0.010309,  0.15306 ,\n",
       "        0.035841,  0.030108, -0.014614,  0.066102,  0.076   ,  0.11524 ,\n",
       "        0.028667,  0.039088,  0.079937, -0.014224,  0.034024,  0.005722,\n",
       "        0.062873,  0.004032, -0.02008 , -0.012022,  0.088889, -0.028061,\n",
       "        0.20499 ,  0.003279, -0.028322,  0.048543, -0.10515 , -0.035971,\n",
       "       -0.00709 ,  0.033962,  0.10219 , -0.00298 , -0.033333, -0.17241 ,\n",
       "        0.012917,  0.045455, -0.13044 , -0.031091,  0.04717 ,  0.099099,\n",
       "       -0.022131,  0.088608, -0.069767, -0.17083 ,  0.01809 , -0.044776,\n",
       "        0.091667,  0.076923, -0.044643, -0.057944, -0.1     ,  0.05    ,\n",
       "       -0.37249 , -0.17949 ,  0.26042 ,  0.099174, -0.093233,  0.2521  ,\n",
       "        0.11812 ,  0.15152 ,  0.084211, -0.10874 ,  0.13187 , -0.014563,\n",
       "       -0.061084, -0.084656,  0.24856 ,  0.      , -0.02963 ,  0.096154,\n",
       "        0.14298 , -0.069498, -0.029046, -0.087179,  0.023585, -0.041475,\n",
       "       -0.016346, -0.044335,  0.041237,  0.069307,  0.021296, -0.086758,\n",
       "       -0.03    , -0.005208,  0.068063,  0.      , -0.029703, -0.02551 ,\n",
       "        0.036649,  0.010204, -0.085859,  0.038674, -0.035106, -0.10056 ,\n",
       "        0.058385,  0.035714,  0.097701, -0.008377, -0.026738,  0.      ,\n",
       "        0.013187,  0.06044 , -0.16062 , -0.006173, -0.034783, -0.045752,\n",
       "        0.009589,  0.041379,  0.05298 , -0.020126,  0.10458 ,  0.017751,\n",
       "        0.033721,  0.017143, -0.11236 , -0.037975,  0.005263, -0.053333,\n",
       "        0.033803, -0.11111 ,  0.078125,  0.10725 ,  0.033333,  0.12903 ,\n",
       "       -0.058286,  0.012346, -0.085366,  0.06    , -0.013836, -0.006494,\n",
       "        0.21046 ,  0.14286 ,  0.019231,  0.071698, -0.017857, -0.095455,\n",
       "        0.021106, -0.05    ,  0.031579,  0.05102 ,  0.000971,  0.014778,\n",
       "       -0.03301 ,  0.05102 ,  0.12136 , -0.007792,  0.1062  ,  0.008   ,\n",
       "        0.1873  ,  0.      ,  0.060811,  0.085987,  0.03695 ,  0.005714,\n",
       "        0.078409, -0.010638,  0.12634 , -0.063007,  0.069409, -0.019231,\n",
       "       -0.081863,  0.      ,  0.002695,  0.086022, -0.029703, -0.082474,\n",
       "       -0.067416,  0.      ,  0.      , -0.091463,  0.064626,  0.063898,\n",
       "        0.066066, -0.037037,  0.035503,  0.002857,  0.072046,  0.008065,\n",
       "        0.024   , -0.052632, -0.080556,  0.10272 ,  0.058172,  0.031414,\n",
       "       -0.015228, -0.033854, -0.048518,  0.082153,  0.044503,  0.11139 ,\n",
       "        0.18679 ,  0.048356,  0.051661,  0.12983 , -0.026563, -0.13483 ,\n",
       "        0.10575 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that calculates the out of sample R^2\n",
    "def my_R_oos_sq(y_test, y_pred):\n",
    "    Er_squared = (y_test - y_pred)**2\n",
    "\n",
    "    SE = 0\n",
    "    for i in range(len(Er_squared)):\n",
    "        SE = SE + (float(Er_squared[i]))\n",
    "    \n",
    "    SR = 0\n",
    "    for j in range(len(y_test)):\n",
    "        SR = SR + (float(y_test[j]))**2\n",
    "    \n",
    "    R_oos_sq = 1 - (SE/SR)\n",
    "    return R_oos_sq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to be optimzed\n",
    "\n",
    "lr = [0.1, 0.01, 0.001]\n",
    "optimizer = ['adam', 'SGD', 'RMSprop']\n",
    "kernel_initializer = ['normal', 'uniform']\n",
    "batch_size = [15, 30, 60]\n",
    "epochs = [100, 150, 200]\n",
    "#activation = ['relu', 'elu', 'tanh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_regressor(optimizer = optimizer, lr = lr, kernel_initializer = kernel_initializer):\n",
    "    # Initialising the ANN\n",
    "    regressor = Sequential()\n",
    "    # Adding the input layer and the first hidden layer\n",
    "    regressor.add(Dense(units = 32, kernel_initializer = kernel_initializer, activation = 'relu', input_dim = 6))\n",
    "    # Adding dropout for NN training...Should comment out in out of sample\n",
    "    regressor.add(Dropout(0.5))\n",
    "    # Adding the second hidden layer\n",
    "    regressor.add(Dense(units = 16, kernel_initializer = kernel_initializer, activation = 'relu'))\n",
    "    # Adding dropout for NN training...Should comment out in out of sample\n",
    "    regressor.add(Dropout(0.5))\n",
    "    # Adding the third hidden layer\n",
    "    regressor.add(Dense(units = 8, kernel_initializer = kernel_initializer, activation = 'relu'))\n",
    "    # Adding the output layer\n",
    "    regressor.add(Dense(units = 1, kernel_initializer = kernel_initializer, activation = 'softsign'))\n",
    "    # Compiling the ANN\n",
    "    regressor.compile(optimizer = optimizer, loss = 'mean_squared_error')\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = KerasRegressor(build_fn = build_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regressor.fit(X_oostrain, y_oostrain, batch_size = 30, epochs = 150)\n",
    "#y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "259/259 [==============================] - 0s 2ms/step - loss: 0.0072\n",
      "Epoch 2/150\n",
      "259/259 [==============================] - 0s 198us/step - loss: 0.0072\n",
      "Epoch 3/150\n",
      "259/259 [==============================] - 0s 180us/step - loss: 0.0073\n",
      "Epoch 4/150\n",
      "259/259 [==============================] - 0s 178us/step - loss: 0.0070\n",
      "Epoch 5/150\n",
      "259/259 [==============================] - 0s 181us/step - loss: 0.0069\n",
      "Epoch 6/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0069\n",
      "Epoch 7/150\n",
      "259/259 [==============================] - 0s 180us/step - loss: 0.0069\n",
      "Epoch 8/150\n",
      "259/259 [==============================] - 0s 173us/step - loss: 0.0067\n",
      "Epoch 9/150\n",
      "259/259 [==============================] - 0s 175us/step - loss: 0.0069\n",
      "Epoch 10/150\n",
      "259/259 [==============================] - 0s 174us/step - loss: 0.0068\n",
      "Epoch 11/150\n",
      "259/259 [==============================] - 0s 177us/step - loss: 0.0069\n",
      "Epoch 12/150\n",
      "259/259 [==============================] - 0s 172us/step - loss: 0.0068\n",
      "Epoch 13/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0069\n",
      "Epoch 14/150\n",
      "259/259 [==============================] - 0s 173us/step - loss: 0.0069\n",
      "Epoch 15/150\n",
      "259/259 [==============================] - 0s 173us/step - loss: 0.0068\n",
      "Epoch 16/150\n",
      "259/259 [==============================] - 0s 171us/step - loss: 0.0068\n",
      "Epoch 17/150\n",
      "259/259 [==============================] - 0s 172us/step - loss: 0.0070\n",
      "Epoch 18/150\n",
      "259/259 [==============================] - 0s 174us/step - loss: 0.0067\n",
      "Epoch 19/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0070\n",
      "Epoch 20/150\n",
      "259/259 [==============================] - 0s 170us/step - loss: 0.0068\n",
      "Epoch 21/150\n",
      "259/259 [==============================] - 0s 169us/step - loss: 0.0067\n",
      "Epoch 22/150\n",
      "259/259 [==============================] - 0s 172us/step - loss: 0.0067\n",
      "Epoch 23/150\n",
      "259/259 [==============================] - 0s 174us/step - loss: 0.0068\n",
      "Epoch 24/150\n",
      "259/259 [==============================] - 0s 169us/step - loss: 0.0067\n",
      "Epoch 25/150\n",
      "259/259 [==============================] - 0s 172us/step - loss: 0.0068\n",
      "Epoch 26/150\n",
      "259/259 [==============================] - 0s 170us/step - loss: 0.0068\n",
      "Epoch 27/150\n",
      "259/259 [==============================] - 0s 170us/step - loss: 0.0067\n",
      "Epoch 28/150\n",
      "259/259 [==============================] - 0s 172us/step - loss: 0.0068\n",
      "Epoch 29/150\n",
      "259/259 [==============================] - 0s 168us/step - loss: 0.0068\n",
      "Epoch 30/150\n",
      "259/259 [==============================] - 0s 171us/step - loss: 0.0068\n",
      "Epoch 31/150\n",
      "259/259 [==============================] - 0s 169us/step - loss: 0.0068\n",
      "Epoch 32/150\n",
      "259/259 [==============================] - 0s 172us/step - loss: 0.0068\n",
      "Epoch 33/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0067\n",
      "Epoch 34/150\n",
      "259/259 [==============================] - 0s 171us/step - loss: 0.0068\n",
      "Epoch 35/150\n",
      "259/259 [==============================] - 0s 170us/step - loss: 0.0067\n",
      "Epoch 36/150\n",
      "259/259 [==============================] - 0s 173us/step - loss: 0.0068\n",
      "Epoch 37/150\n",
      "259/259 [==============================] - 0s 174us/step - loss: 0.0068\n",
      "Epoch 38/150\n",
      "259/259 [==============================] - 0s 173us/step - loss: 0.0067\n",
      "Epoch 39/150\n",
      "259/259 [==============================] - 0s 171us/step - loss: 0.0068\n",
      "Epoch 40/150\n",
      "259/259 [==============================] - 0s 173us/step - loss: 0.0068\n",
      "Epoch 41/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0067\n",
      "Epoch 42/150\n",
      "259/259 [==============================] - 0s 172us/step - loss: 0.0067\n",
      "Epoch 43/150\n",
      "259/259 [==============================] - 0s 177us/step - loss: 0.0066\n",
      "Epoch 44/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0067\n",
      "Epoch 45/150\n",
      "259/259 [==============================] - 0s 172us/step - loss: 0.0067\n",
      "Epoch 46/150\n",
      "259/259 [==============================] - 0s 175us/step - loss: 0.0067\n",
      "Epoch 47/150\n",
      "259/259 [==============================] - 0s 173us/step - loss: 0.0066\n",
      "Epoch 48/150\n",
      "259/259 [==============================] - 0s 174us/step - loss: 0.0067\n",
      "Epoch 49/150\n",
      "259/259 [==============================] - 0s 173us/step - loss: 0.0066\n",
      "Epoch 50/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0067\n",
      "Epoch 51/150\n",
      "259/259 [==============================] - 0s 169us/step - loss: 0.0066\n",
      "Epoch 52/150\n",
      "259/259 [==============================] - 0s 182us/step - loss: 0.0066\n",
      "Epoch 53/150\n",
      "259/259 [==============================] - 0s 178us/step - loss: 0.0067\n",
      "Epoch 54/150\n",
      "259/259 [==============================] - 0s 178us/step - loss: 0.0068\n",
      "Epoch 55/150\n",
      "259/259 [==============================] - 0s 179us/step - loss: 0.0067\n",
      "Epoch 56/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0067\n",
      "Epoch 57/150\n",
      "259/259 [==============================] - 0s 173us/step - loss: 0.0067\n",
      "Epoch 58/150\n",
      "259/259 [==============================] - 0s 180us/step - loss: 0.0068\n",
      "Epoch 59/150\n",
      "259/259 [==============================] - 0s 177us/step - loss: 0.0067\n",
      "Epoch 60/150\n",
      "259/259 [==============================] - 0s 177us/step - loss: 0.0067\n",
      "Epoch 61/150\n",
      "259/259 [==============================] - 0s 180us/step - loss: 0.0068\n",
      "Epoch 62/150\n",
      "259/259 [==============================] - 0s 179us/step - loss: 0.0068\n",
      "Epoch 63/150\n",
      "259/259 [==============================] - 0s 175us/step - loss: 0.0067\n",
      "Epoch 64/150\n",
      "259/259 [==============================] - 0s 183us/step - loss: 0.0067\n",
      "Epoch 65/150\n",
      "259/259 [==============================] - 0s 178us/step - loss: 0.0068\n",
      "Epoch 66/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0068\n",
      "Epoch 67/150\n",
      "259/259 [==============================] - 0s 180us/step - loss: 0.0066\n",
      "Epoch 68/150\n",
      "259/259 [==============================] - 0s 182us/step - loss: 0.0067\n",
      "Epoch 69/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0068\n",
      "Epoch 70/150\n",
      "259/259 [==============================] - 0s 179us/step - loss: 0.0070\n",
      "Epoch 71/150\n",
      "259/259 [==============================] - 0s 179us/step - loss: 0.0066\n",
      "Epoch 72/150\n",
      "259/259 [==============================] - 0s 180us/step - loss: 0.0067\n",
      "Epoch 73/150\n",
      "259/259 [==============================] - 0s 175us/step - loss: 0.0067\n",
      "Epoch 74/150\n",
      "259/259 [==============================] - 0s 181us/step - loss: 0.0068\n",
      "Epoch 75/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0067\n",
      "Epoch 76/150\n",
      "259/259 [==============================] - 0s 182us/step - loss: 0.0067\n",
      "Epoch 77/150\n",
      "259/259 [==============================] - 0s 182us/step - loss: 0.0068\n",
      "Epoch 78/150\n",
      "259/259 [==============================] - 0s 183us/step - loss: 0.0066\n",
      "Epoch 79/150\n",
      "259/259 [==============================] - 0s 180us/step - loss: 0.0068\n",
      "Epoch 80/150\n",
      "259/259 [==============================] - 0s 179us/step - loss: 0.0066\n",
      "Epoch 81/150\n",
      "259/259 [==============================] - 0s 179us/step - loss: 0.0067\n",
      "Epoch 82/150\n",
      "259/259 [==============================] - 0s 180us/step - loss: 0.0068\n",
      "Epoch 83/150\n",
      "259/259 [==============================] - 0s 174us/step - loss: 0.0067\n",
      "Epoch 84/150\n",
      "259/259 [==============================] - 0s 180us/step - loss: 0.0067\n",
      "Epoch 85/150\n",
      "259/259 [==============================] - 0s 181us/step - loss: 0.0066\n",
      "Epoch 86/150\n",
      "259/259 [==============================] - 0s 177us/step - loss: 0.0066\n",
      "Epoch 87/150\n",
      "259/259 [==============================] - 0s 181us/step - loss: 0.0066\n",
      "Epoch 88/150\n",
      "259/259 [==============================] - 0s 178us/step - loss: 0.0068\n",
      "Epoch 89/150\n",
      "259/259 [==============================] - 0s 182us/step - loss: 0.0067\n",
      "Epoch 90/150\n",
      "259/259 [==============================] - 0s 175us/step - loss: 0.0067\n",
      "Epoch 91/150\n",
      "259/259 [==============================] - 0s 181us/step - loss: 0.0065\n",
      "Epoch 92/150\n",
      "259/259 [==============================] - 0s 180us/step - loss: 0.0065\n",
      "Epoch 93/150\n",
      "259/259 [==============================] - 0s 181us/step - loss: 0.0066\n",
      "Epoch 94/150\n",
      "259/259 [==============================] - 0s 177us/step - loss: 0.0066\n",
      "Epoch 95/150\n",
      "259/259 [==============================] - 0s 178us/step - loss: 0.0068\n",
      "Epoch 96/150\n",
      "259/259 [==============================] - 0s 181us/step - loss: 0.0067\n",
      "Epoch 97/150\n",
      "259/259 [==============================] - 0s 179us/step - loss: 0.0067\n",
      "Epoch 98/150\n",
      "259/259 [==============================] - 0s 178us/step - loss: 0.0066\n",
      "Epoch 99/150\n",
      "259/259 [==============================] - 0s 175us/step - loss: 0.0067\n",
      "Epoch 100/150\n",
      "259/259 [==============================] - 0s 173us/step - loss: 0.0066\n",
      "Epoch 101/150\n",
      "259/259 [==============================] - 0s 173us/step - loss: 0.0066\n",
      "Epoch 102/150\n",
      "259/259 [==============================] - 0s 175us/step - loss: 0.0067\n",
      "Epoch 103/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0067\n",
      "Epoch 104/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0066\n",
      "Epoch 105/150\n",
      "259/259 [==============================] - 0s 173us/step - loss: 0.0068\n",
      "Epoch 106/150\n",
      "259/259 [==============================] - 0s 172us/step - loss: 0.0067\n",
      "Epoch 107/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0068\n",
      "Epoch 108/150\n",
      "259/259 [==============================] - 0s 178us/step - loss: 0.0067\n",
      "Epoch 109/150\n",
      "259/259 [==============================] - 0s 180us/step - loss: 0.0067\n",
      "Epoch 110/150\n",
      "259/259 [==============================] - 0s 178us/step - loss: 0.0067\n",
      "Epoch 111/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0066\n",
      "Epoch 112/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0064\n",
      "Epoch 113/150\n",
      "259/259 [==============================] - 0s 179us/step - loss: 0.0068\n",
      "Epoch 114/150\n",
      "259/259 [==============================] - 0s 180us/step - loss: 0.0065\n",
      "Epoch 115/150\n",
      "259/259 [==============================] - 0s 179us/step - loss: 0.0066\n",
      "Epoch 116/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0066\n",
      "Epoch 117/150\n",
      "259/259 [==============================] - 0s 175us/step - loss: 0.0067\n",
      "Epoch 118/150\n",
      "259/259 [==============================] - 0s 178us/step - loss: 0.0066\n",
      "Epoch 119/150\n",
      "259/259 [==============================] - 0s 177us/step - loss: 0.0066\n",
      "Epoch 120/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0067\n",
      "Epoch 121/150\n",
      "259/259 [==============================] - 0s 179us/step - loss: 0.0067\n",
      "Epoch 122/150\n",
      "259/259 [==============================] - 0s 178us/step - loss: 0.0066\n",
      "Epoch 123/150\n",
      "259/259 [==============================] - 0s 175us/step - loss: 0.0067\n",
      "Epoch 124/150\n",
      "259/259 [==============================] - 0s 179us/step - loss: 0.0067\n",
      "Epoch 125/150\n",
      "259/259 [==============================] - 0s 180us/step - loss: 0.0068\n",
      "Epoch 126/150\n",
      "259/259 [==============================] - 0s 177us/step - loss: 0.0066\n",
      "Epoch 127/150\n",
      "259/259 [==============================] - 0s 179us/step - loss: 0.0067\n",
      "Epoch 128/150\n",
      "259/259 [==============================] - 0s 177us/step - loss: 0.0066\n",
      "Epoch 129/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0064\n",
      "Epoch 130/150\n",
      "259/259 [==============================] - 0s 178us/step - loss: 0.0067\n",
      "Epoch 131/150\n",
      "259/259 [==============================] - 0s 177us/step - loss: 0.0067\n",
      "Epoch 132/150\n",
      "259/259 [==============================] - 0s 178us/step - loss: 0.0069\n",
      "Epoch 133/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0065\n",
      "Epoch 134/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0068\n",
      "Epoch 135/150\n",
      "259/259 [==============================] - 0s 177us/step - loss: 0.0065\n",
      "Epoch 136/150\n",
      "259/259 [==============================] - 0s 178us/step - loss: 0.0064\n",
      "Epoch 137/150\n",
      "259/259 [==============================] - 0s 176us/step - loss: 0.0065\n",
      "Epoch 138/150\n",
      "259/259 [==============================] - 0s 183us/step - loss: 0.0064\n",
      "Epoch 139/150\n",
      "259/259 [==============================] - 0s 180us/step - loss: 0.0066\n",
      "Epoch 140/150\n",
      "259/259 [==============================] - 0s 179us/step - loss: 0.0066\n",
      "Epoch 141/150\n",
      "259/259 [==============================] - 0s 182us/step - loss: 0.0065\n",
      "Epoch 142/150\n",
      "259/259 [==============================] - 0s 185us/step - loss: 0.0065\n",
      "Epoch 143/150\n",
      "259/259 [==============================] - 0s 183us/step - loss: 0.0065\n",
      "Epoch 144/150\n",
      "259/259 [==============================] - 0s 181us/step - loss: 0.0067\n",
      "Epoch 145/150\n",
      "259/259 [==============================] - 0s 182us/step - loss: 0.0066\n",
      "Epoch 146/150\n",
      "259/259 [==============================] - 0s 184us/step - loss: 0.0064\n",
      "Epoch 147/150\n",
      "259/259 [==============================] - 0s 181us/step - loss: 0.0064\n",
      "Epoch 148/150\n",
      "259/259 [==============================] - 0s 180us/step - loss: 0.0067\n",
      "Epoch 149/150\n",
      "259/259 [==============================] - 0s 180us/step - loss: 0.0066\n",
      "Epoch 150/150\n",
      "259/259 [==============================] - 0s 181us/step - loss: 0.0066\n",
      "Best: -0.011441 using {'batch_size': 30, 'epochs': 150, 'kernel_initializer': 'uniform', 'optimizer': 'SGD'}\n",
      "-1.850504 (1.928097) with: {'batch_size': 15, 'epochs': 100, 'kernel_initializer': 'normal', 'optimizer': 'adam'}\n",
      "-0.321844 (0.311050) with: {'batch_size': 15, 'epochs': 100, 'kernel_initializer': 'normal', 'optimizer': 'SGD'}\n",
      "-1.537643 (1.139050) with: {'batch_size': 15, 'epochs': 100, 'kernel_initializer': 'normal', 'optimizer': 'RMSprop'}\n",
      "-1.774145 (0.979382) with: {'batch_size': 15, 'epochs': 100, 'kernel_initializer': 'uniform', 'optimizer': 'adam'}\n",
      "-0.032414 (0.038755) with: {'batch_size': 15, 'epochs': 100, 'kernel_initializer': 'uniform', 'optimizer': 'SGD'}\n",
      "-1.536754 (0.907931) with: {'batch_size': 15, 'epochs': 100, 'kernel_initializer': 'uniform', 'optimizer': 'RMSprop'}\n",
      "-2.008242 (1.656901) with: {'batch_size': 15, 'epochs': 150, 'kernel_initializer': 'normal', 'optimizer': 'adam'}\n",
      "-0.373946 (0.221402) with: {'batch_size': 15, 'epochs': 150, 'kernel_initializer': 'normal', 'optimizer': 'SGD'}\n",
      "-1.657500 (0.823059) with: {'batch_size': 15, 'epochs': 150, 'kernel_initializer': 'normal', 'optimizer': 'RMSprop'}\n",
      "-1.887044 (1.256718) with: {'batch_size': 15, 'epochs': 150, 'kernel_initializer': 'uniform', 'optimizer': 'adam'}\n",
      "-0.015746 (0.055615) with: {'batch_size': 15, 'epochs': 150, 'kernel_initializer': 'uniform', 'optimizer': 'SGD'}\n",
      "-1.749426 (0.804450) with: {'batch_size': 15, 'epochs': 150, 'kernel_initializer': 'uniform', 'optimizer': 'RMSprop'}\n",
      "-2.040855 (1.119654) with: {'batch_size': 15, 'epochs': 200, 'kernel_initializer': 'normal', 'optimizer': 'adam'}\n",
      "-0.322037 (0.358532) with: {'batch_size': 15, 'epochs': 200, 'kernel_initializer': 'normal', 'optimizer': 'SGD'}\n",
      "-2.233193 (1.560712) with: {'batch_size': 15, 'epochs': 200, 'kernel_initializer': 'normal', 'optimizer': 'RMSprop'}\n",
      "-1.775521 (1.249658) with: {'batch_size': 15, 'epochs': 200, 'kernel_initializer': 'uniform', 'optimizer': 'adam'}\n",
      "-0.050485 (0.092862) with: {'batch_size': 15, 'epochs': 200, 'kernel_initializer': 'uniform', 'optimizer': 'SGD'}\n",
      "-2.372071 (1.324062) with: {'batch_size': 15, 'epochs': 200, 'kernel_initializer': 'uniform', 'optimizer': 'RMSprop'}\n",
      "-1.061439 (1.120851) with: {'batch_size': 30, 'epochs': 100, 'kernel_initializer': 'normal', 'optimizer': 'adam'}\n",
      "-0.206452 (0.180344) with: {'batch_size': 30, 'epochs': 100, 'kernel_initializer': 'normal', 'optimizer': 'SGD'}\n",
      "-1.367670 (1.144014) with: {'batch_size': 30, 'epochs': 100, 'kernel_initializer': 'normal', 'optimizer': 'RMSprop'}\n",
      "-1.579489 (1.162939) with: {'batch_size': 30, 'epochs': 100, 'kernel_initializer': 'uniform', 'optimizer': 'adam'}\n",
      "-0.045952 (0.038854) with: {'batch_size': 30, 'epochs': 100, 'kernel_initializer': 'uniform', 'optimizer': 'SGD'}\n",
      "-1.647214 (0.928963) with: {'batch_size': 30, 'epochs': 100, 'kernel_initializer': 'uniform', 'optimizer': 'RMSprop'}\n",
      "-1.623411 (1.219338) with: {'batch_size': 30, 'epochs': 150, 'kernel_initializer': 'normal', 'optimizer': 'adam'}\n",
      "-0.224652 (0.118470) with: {'batch_size': 30, 'epochs': 150, 'kernel_initializer': 'normal', 'optimizer': 'SGD'}\n",
      "-1.843668 (1.149967) with: {'batch_size': 30, 'epochs': 150, 'kernel_initializer': 'normal', 'optimizer': 'RMSprop'}\n",
      "-2.060234 (1.263428) with: {'batch_size': 30, 'epochs': 150, 'kernel_initializer': 'uniform', 'optimizer': 'adam'}\n",
      "-0.011441 (0.038563) with: {'batch_size': 30, 'epochs': 150, 'kernel_initializer': 'uniform', 'optimizer': 'SGD'}\n",
      "-2.246177 (1.112626) with: {'batch_size': 30, 'epochs': 150, 'kernel_initializer': 'uniform', 'optimizer': 'RMSprop'}\n",
      "-2.048324 (1.220891) with: {'batch_size': 30, 'epochs': 200, 'kernel_initializer': 'normal', 'optimizer': 'adam'}\n",
      "-0.277779 (0.219446) with: {'batch_size': 30, 'epochs': 200, 'kernel_initializer': 'normal', 'optimizer': 'SGD'}\n",
      "-1.882680 (1.085525) with: {'batch_size': 30, 'epochs': 200, 'kernel_initializer': 'normal', 'optimizer': 'RMSprop'}\n",
      "-1.877939 (1.028121) with: {'batch_size': 30, 'epochs': 200, 'kernel_initializer': 'uniform', 'optimizer': 'adam'}\n",
      "-0.015298 (0.046312) with: {'batch_size': 30, 'epochs': 200, 'kernel_initializer': 'uniform', 'optimizer': 'SGD'}\n",
      "-1.861129 (1.070506) with: {'batch_size': 30, 'epochs': 200, 'kernel_initializer': 'uniform', 'optimizer': 'RMSprop'}\n",
      "-1.324486 (1.088781) with: {'batch_size': 60, 'epochs': 100, 'kernel_initializer': 'normal', 'optimizer': 'adam'}\n",
      "-0.162891 (0.178825) with: {'batch_size': 60, 'epochs': 100, 'kernel_initializer': 'normal', 'optimizer': 'SGD'}\n",
      "-1.091563 (1.024386) with: {'batch_size': 60, 'epochs': 100, 'kernel_initializer': 'normal', 'optimizer': 'RMSprop'}\n",
      "-1.875670 (1.860357) with: {'batch_size': 60, 'epochs': 100, 'kernel_initializer': 'uniform', 'optimizer': 'adam'}\n",
      "-0.054665 (0.022185) with: {'batch_size': 60, 'epochs': 100, 'kernel_initializer': 'uniform', 'optimizer': 'SGD'}\n",
      "-1.308704 (0.941230) with: {'batch_size': 60, 'epochs': 100, 'kernel_initializer': 'uniform', 'optimizer': 'RMSprop'}\n",
      "-1.888123 (2.322534) with: {'batch_size': 60, 'epochs': 150, 'kernel_initializer': 'normal', 'optimizer': 'adam'}\n",
      "-0.182243 (0.209662) with: {'batch_size': 60, 'epochs': 150, 'kernel_initializer': 'normal', 'optimizer': 'SGD'}\n",
      "-1.333141 (0.771631) with: {'batch_size': 60, 'epochs': 150, 'kernel_initializer': 'normal', 'optimizer': 'RMSprop'}\n",
      "-1.819869 (1.280263) with: {'batch_size': 60, 'epochs': 150, 'kernel_initializer': 'uniform', 'optimizer': 'adam'}\n",
      "-0.058499 (0.044389) with: {'batch_size': 60, 'epochs': 150, 'kernel_initializer': 'uniform', 'optimizer': 'SGD'}\n",
      "-1.324080 (0.650732) with: {'batch_size': 60, 'epochs': 150, 'kernel_initializer': 'uniform', 'optimizer': 'RMSprop'}\n",
      "-2.090430 (2.160799) with: {'batch_size': 60, 'epochs': 200, 'kernel_initializer': 'normal', 'optimizer': 'adam'}\n",
      "-0.086275 (0.125927) with: {'batch_size': 60, 'epochs': 200, 'kernel_initializer': 'normal', 'optimizer': 'SGD'}\n",
      "-1.417443 (0.735180) with: {'batch_size': 60, 'epochs': 200, 'kernel_initializer': 'normal', 'optimizer': 'RMSprop'}\n",
      "-2.157862 (1.393086) with: {'batch_size': 60, 'epochs': 200, 'kernel_initializer': 'uniform', 'optimizer': 'adam'}\n",
      "-0.018905 (0.032207) with: {'batch_size': 60, 'epochs': 200, 'kernel_initializer': 'uniform', 'optimizer': 'SGD'}\n",
      "-1.676986 (0.779882) with: {'batch_size': 60, 'epochs': 200, 'kernel_initializer': 'uniform', 'optimizer': 'RMSprop'}\n"
     ]
    }
   ],
   "source": [
    "# Grid Search for hyperparameters optimization\n",
    "\n",
    "param_grid = dict(optimizer = optimizer, kernel_initializer = kernel_initializer, batch_size = batch_size, epochs = epochs)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits = 5)\n",
    "grid_search = GridSearchCV(estimator = regressor, param_grid = param_grid, scoring = 'r2', cv = tscv, n_jobs = -1)\n",
    "grid_search = grid_search.fit(X = X_train, y = y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_search.best_score_, grid_search.best_params_))\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04020058046171093"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the R Squared out of sample\n",
    "my_R_oos_sq(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f2cb35cff84d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Output Binary Classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mPred_NN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mPred_NN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Output Binary Classification\n",
    "Pred_NN = []\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] >= 0:\n",
    "        Pred_NN.append(1)\n",
    "    else:\n",
    "        Pred_NN.append(0)\n",
    "        \n",
    "Val = []\n",
    "for j in range(len(y_test)):\n",
    "    if y_test[j] >= 0:\n",
    "        Val.append(1)\n",
    "    else:\n",
    "        Val.append(0)\n",
    "        \n",
    "count = 0\n",
    "for k in range(len(y_pred)):\n",
    "    if Pred_NN[k] == Val[k]:\n",
    "        count += 1\n",
    "res_NN = count/len(y_pred)\n",
    "res_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_oostrain = np.reshape(X_oostrain, (X_oostrain.shape[0], 1, X_oostrain.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(518, 1, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_oostrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LSTMregressor():\n",
    "    LSTMregressor = Sequential()\n",
    "    LSTMregressor.add(LSTM(units = 60, input_shape = (1, 6)))\n",
    "    LSTMregressor.add(Dense(units = 1, activation = 'softsign'))\n",
    "    LSTMregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "    return LSTMregressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMregressor = KerasRegressor(build_fn = build_LSTMregressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "518/518 [==============================] - 1s 2ms/step - loss: 0.0101\n",
      "Epoch 2/100\n",
      "518/518 [==============================] - 0s 168us/step - loss: 0.0071\n",
      "Epoch 3/100\n",
      "518/518 [==============================] - 0s 173us/step - loss: 0.0069\n",
      "Epoch 4/100\n",
      "518/518 [==============================] - 0s 184us/step - loss: 0.0068\n",
      "Epoch 5/100\n",
      "518/518 [==============================] - 0s 173us/step - loss: 0.0067\n",
      "Epoch 6/100\n",
      "518/518 [==============================] - 0s 168us/step - loss: 0.0068\n",
      "Epoch 7/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0067\n",
      "Epoch 8/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0067\n",
      "Epoch 9/100\n",
      "518/518 [==============================] - 0s 167us/step - loss: 0.0067\n",
      "Epoch 10/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0067\n",
      "Epoch 11/100\n",
      "518/518 [==============================] - 0s 166us/step - loss: 0.0067\n",
      "Epoch 12/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0067\n",
      "Epoch 13/100\n",
      "518/518 [==============================] - 0s 168us/step - loss: 0.0066\n",
      "Epoch 14/100\n",
      "518/518 [==============================] - 0s 166us/step - loss: 0.0066\n",
      "Epoch 15/100\n",
      "518/518 [==============================] - 0s 168us/step - loss: 0.0066\n",
      "Epoch 16/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0066\n",
      "Epoch 17/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0066\n",
      "Epoch 18/100\n",
      "518/518 [==============================] - 0s 171us/step - loss: 0.0066\n",
      "Epoch 19/100\n",
      "518/518 [==============================] - 0s 170us/step - loss: 0.0067\n",
      "Epoch 20/100\n",
      "518/518 [==============================] - 0s 162us/step - loss: 0.0066\n",
      "Epoch 21/100\n",
      "518/518 [==============================] - 0s 167us/step - loss: 0.0066\n",
      "Epoch 22/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0065\n",
      "Epoch 23/100\n",
      "518/518 [==============================] - 0s 166us/step - loss: 0.0065\n",
      "Epoch 24/100\n",
      "518/518 [==============================] - 0s 166us/step - loss: 0.0065\n",
      "Epoch 25/100\n",
      "518/518 [==============================] - 0s 168us/step - loss: 0.0065\n",
      "Epoch 26/100\n",
      "518/518 [==============================] - 0s 166us/step - loss: 0.0066\n",
      "Epoch 27/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0066\n",
      "Epoch 28/100\n",
      "518/518 [==============================] - 0s 164us/step - loss: 0.0065\n",
      "Epoch 29/100\n",
      "518/518 [==============================] - 0s 169us/step - loss: 0.0066\n",
      "Epoch 30/100\n",
      "518/518 [==============================] - 0s 167us/step - loss: 0.0065\n",
      "Epoch 31/100\n",
      "518/518 [==============================] - 0s 170us/step - loss: 0.0067\n",
      "Epoch 32/100\n",
      "518/518 [==============================] - 0s 166us/step - loss: 0.0066\n",
      "Epoch 33/100\n",
      "518/518 [==============================] - 0s 163us/step - loss: 0.0066\n",
      "Epoch 34/100\n",
      "518/518 [==============================] - 0s 167us/step - loss: 0.0066\n",
      "Epoch 35/100\n",
      "518/518 [==============================] - 0s 166us/step - loss: 0.0067\n",
      "Epoch 36/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0066\n",
      "Epoch 37/100\n",
      "518/518 [==============================] - 0s 166us/step - loss: 0.0065\n",
      "Epoch 38/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0066\n",
      "Epoch 39/100\n",
      "518/518 [==============================] - 0s 166us/step - loss: 0.0066\n",
      "Epoch 40/100\n",
      "518/518 [==============================] - 0s 164us/step - loss: 0.0065\n",
      "Epoch 41/100\n",
      "518/518 [==============================] - 0s 166us/step - loss: 0.0066\n",
      "Epoch 42/100\n",
      "518/518 [==============================] - 0s 167us/step - loss: 0.0065\n",
      "Epoch 43/100\n",
      "518/518 [==============================] - 0s 167us/step - loss: 0.0065\n",
      "Epoch 44/100\n",
      "518/518 [==============================] - 0s 168us/step - loss: 0.0065\n",
      "Epoch 45/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0065\n",
      "Epoch 46/100\n",
      "518/518 [==============================] - 0s 170us/step - loss: 0.0065\n",
      "Epoch 47/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0066\n",
      "Epoch 48/100\n",
      "518/518 [==============================] - 0s 167us/step - loss: 0.0065\n",
      "Epoch 49/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0065\n",
      "Epoch 50/100\n",
      "518/518 [==============================] - 0s 168us/step - loss: 0.0065\n",
      "Epoch 51/100\n",
      "518/518 [==============================] - 0s 163us/step - loss: 0.0065\n",
      "Epoch 52/100\n",
      "518/518 [==============================] - 0s 162us/step - loss: 0.0065\n",
      "Epoch 53/100\n",
      "518/518 [==============================] - 0s 167us/step - loss: 0.0065\n",
      "Epoch 54/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0065\n",
      "Epoch 55/100\n",
      "518/518 [==============================] - 0s 164us/step - loss: 0.0064\n",
      "Epoch 56/100\n",
      "518/518 [==============================] - 0s 160us/step - loss: 0.0066\n",
      "Epoch 57/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0065\n",
      "Epoch 58/100\n",
      "518/518 [==============================] - 0s 161us/step - loss: 0.0065\n",
      "Epoch 59/100\n",
      "518/518 [==============================] - 0s 164us/step - loss: 0.0065\n",
      "Epoch 60/100\n",
      "518/518 [==============================] - 0s 163us/step - loss: 0.0065\n",
      "Epoch 61/100\n",
      "518/518 [==============================] - 0s 163us/step - loss: 0.0066\n",
      "Epoch 62/100\n",
      "518/518 [==============================] - 0s 164us/step - loss: 0.0065\n",
      "Epoch 63/100\n",
      "518/518 [==============================] - 0s 163us/step - loss: 0.0065\n",
      "Epoch 64/100\n",
      "518/518 [==============================] - 0s 164us/step - loss: 0.0064\n",
      "Epoch 65/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0065\n",
      "Epoch 66/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0064\n",
      "Epoch 67/100\n",
      "518/518 [==============================] - 0s 167us/step - loss: 0.0064\n",
      "Epoch 68/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0065\n",
      "Epoch 69/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0064\n",
      "Epoch 70/100\n",
      "518/518 [==============================] - 0s 169us/step - loss: 0.0064\n",
      "Epoch 71/100\n",
      "518/518 [==============================] - 0s 167us/step - loss: 0.0064\n",
      "Epoch 72/100\n",
      "518/518 [==============================] - 0s 167us/step - loss: 0.0064\n",
      "Epoch 73/100\n",
      "518/518 [==============================] - 0s 163us/step - loss: 0.0064\n",
      "Epoch 74/100\n",
      "518/518 [==============================] - 0s 167us/step - loss: 0.0065\n",
      "Epoch 75/100\n",
      "518/518 [==============================] - 0s 167us/step - loss: 0.0064\n",
      "Epoch 76/100\n",
      "518/518 [==============================] - 0s 169us/step - loss: 0.0064\n",
      "Epoch 77/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0063\n",
      "Epoch 78/100\n",
      "518/518 [==============================] - 0s 167us/step - loss: 0.0064\n",
      "Epoch 79/100\n",
      "518/518 [==============================] - 0s 167us/step - loss: 0.0065\n",
      "Epoch 80/100\n",
      "518/518 [==============================] - 0s 167us/step - loss: 0.0064\n",
      "Epoch 81/100\n",
      "518/518 [==============================] - 0s 164us/step - loss: 0.0065\n",
      "Epoch 82/100\n",
      "518/518 [==============================] - 0s 167us/step - loss: 0.0064\n",
      "Epoch 83/100\n",
      "518/518 [==============================] - 0s 168us/step - loss: 0.0064\n",
      "Epoch 84/100\n",
      "518/518 [==============================] - 0s 167us/step - loss: 0.0064\n",
      "Epoch 85/100\n",
      "518/518 [==============================] - 0s 164us/step - loss: 0.0064\n",
      "Epoch 86/100\n",
      "518/518 [==============================] - 0s 166us/step - loss: 0.0064\n",
      "Epoch 87/100\n",
      "518/518 [==============================] - 0s 165us/step - loss: 0.0064\n",
      "Epoch 88/100\n",
      "518/518 [==============================] - 0s 168us/step - loss: 0.0064\n",
      "Epoch 89/100\n",
      "518/518 [==============================] - 0s 173us/step - loss: 0.0064\n",
      "Epoch 90/100\n",
      "518/518 [==============================] - 0s 171us/step - loss: 0.0064\n",
      "Epoch 91/100\n",
      "518/518 [==============================] - 0s 174us/step - loss: 0.0064\n",
      "Epoch 92/100\n",
      "518/518 [==============================] - 0s 171us/step - loss: 0.0064\n",
      "Epoch 93/100\n",
      "518/518 [==============================] - 0s 178us/step - loss: 0.0064\n",
      "Epoch 94/100\n",
      "518/518 [==============================] - 0s 177us/step - loss: 0.0064\n",
      "Epoch 95/100\n",
      "518/518 [==============================] - 0s 180us/step - loss: 0.0064\n",
      "Epoch 96/100\n",
      "518/518 [==============================] - 0s 176us/step - loss: 0.0064\n",
      "Epoch 97/100\n",
      "518/518 [==============================] - 0s 177us/step - loss: 0.0064\n",
      "Epoch 98/100\n",
      "518/518 [==============================] - 0s 174us/step - loss: 0.0064\n",
      "Epoch 99/100\n",
      "518/518 [==============================] - 0s 175us/step - loss: 0.0064\n",
      "Epoch 100/100\n",
      "518/518 [==============================] - 0s 174us/step - loss: 0.0064\n"
     ]
    }
   ],
   "source": [
    "LSTMregressor.fit(X_oostrain, y_oostrain, batch_size = 30, epochs = 100)\n",
    "y_pred_LSTM = LSTMregressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07998170363518631"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the R Squared out of sample\n",
    "my_R_oos_sq(y_test, y_pred_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5038759689922481"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output Binary Classification\n",
    "Pred_LSTM = []\n",
    "for i in range(len(y_pred_LSTM)):\n",
    "    if y_pred_LSTM[i] >= 0:\n",
    "        Pred_LSTM.append(1)\n",
    "    else:\n",
    "        Pred_LSTM.append(0)\n",
    "    \n",
    "Val = []\n",
    "for j in range(len(y_test)):\n",
    "    if y_test[j] >= 0:\n",
    "        Val.append(1)\n",
    "    else:\n",
    "        Val.append(0)\n",
    "    \n",
    "count = 0\n",
    "for k in range(len(y_pred_LSTM)):\n",
    "    if Pred_LSTM[k] == Val[k]:\n",
    "        count += 1\n",
    "res_LSTM = count/len(y_pred_LSTM)\n",
    "res_LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(n_estimators = 1000, criterion = 'mse', \n",
    "                            min_samples_split = 40, min_samples_leaf = 10, random_state = 69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_oostrain, y_oostrain)\n",
    "y_pred_r = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.058383 using {'criterion': 'mse', 'min_samples_leaf': 10, 'min_samples_split': 40, 'n_estimators': 1000}\n",
      "-0.206955 (0.131810) with: {'criterion': 'mse', 'min_samples_leaf': 1, 'min_samples_split': 20, 'n_estimators': 500}\n",
      "-0.198339 (0.122959) with: {'criterion': 'mse', 'min_samples_leaf': 1, 'min_samples_split': 20, 'n_estimators': 1000}\n",
      "-0.182466 (0.126269) with: {'criterion': 'mse', 'min_samples_leaf': 1, 'min_samples_split': 25, 'n_estimators': 500}\n",
      "-0.175007 (0.118510) with: {'criterion': 'mse', 'min_samples_leaf': 1, 'min_samples_split': 25, 'n_estimators': 1000}\n",
      "-0.147912 (0.134094) with: {'criterion': 'mse', 'min_samples_leaf': 1, 'min_samples_split': 30, 'n_estimators': 500}\n",
      "-0.142500 (0.126059) with: {'criterion': 'mse', 'min_samples_leaf': 1, 'min_samples_split': 30, 'n_estimators': 1000}\n",
      "-0.132541 (0.126506) with: {'criterion': 'mse', 'min_samples_leaf': 1, 'min_samples_split': 35, 'n_estimators': 500}\n",
      "-0.127784 (0.121121) with: {'criterion': 'mse', 'min_samples_leaf': 1, 'min_samples_split': 35, 'n_estimators': 1000}\n",
      "-0.121769 (0.120396) with: {'criterion': 'mse', 'min_samples_leaf': 1, 'min_samples_split': 40, 'n_estimators': 500}\n",
      "-0.118010 (0.116014) with: {'criterion': 'mse', 'min_samples_leaf': 1, 'min_samples_split': 40, 'n_estimators': 1000}\n",
      "-0.165517 (0.134666) with: {'criterion': 'mse', 'min_samples_leaf': 3, 'min_samples_split': 20, 'n_estimators': 500}\n",
      "-0.159454 (0.127926) with: {'criterion': 'mse', 'min_samples_leaf': 3, 'min_samples_split': 20, 'n_estimators': 1000}\n",
      "-0.147496 (0.128401) with: {'criterion': 'mse', 'min_samples_leaf': 3, 'min_samples_split': 25, 'n_estimators': 500}\n",
      "-0.144255 (0.121786) with: {'criterion': 'mse', 'min_samples_leaf': 3, 'min_samples_split': 25, 'n_estimators': 1000}\n",
      "-0.118970 (0.129332) with: {'criterion': 'mse', 'min_samples_leaf': 3, 'min_samples_split': 30, 'n_estimators': 500}\n",
      "-0.116537 (0.124275) with: {'criterion': 'mse', 'min_samples_leaf': 3, 'min_samples_split': 30, 'n_estimators': 1000}\n",
      "-0.108137 (0.125280) with: {'criterion': 'mse', 'min_samples_leaf': 3, 'min_samples_split': 35, 'n_estimators': 500}\n",
      "-0.105049 (0.120281) with: {'criterion': 'mse', 'min_samples_leaf': 3, 'min_samples_split': 35, 'n_estimators': 1000}\n",
      "-0.100199 (0.118954) with: {'criterion': 'mse', 'min_samples_leaf': 3, 'min_samples_split': 40, 'n_estimators': 500}\n",
      "-0.097854 (0.115643) with: {'criterion': 'mse', 'min_samples_leaf': 3, 'min_samples_split': 40, 'n_estimators': 1000}\n",
      "-0.146460 (0.117954) with: {'criterion': 'mse', 'min_samples_leaf': 5, 'min_samples_split': 20, 'n_estimators': 500}\n",
      "-0.140760 (0.112747) with: {'criterion': 'mse', 'min_samples_leaf': 5, 'min_samples_split': 20, 'n_estimators': 1000}\n",
      "-0.131691 (0.115257) with: {'criterion': 'mse', 'min_samples_leaf': 5, 'min_samples_split': 25, 'n_estimators': 500}\n",
      "-0.126914 (0.107142) with: {'criterion': 'mse', 'min_samples_leaf': 5, 'min_samples_split': 25, 'n_estimators': 1000}\n",
      "-0.104126 (0.117960) with: {'criterion': 'mse', 'min_samples_leaf': 5, 'min_samples_split': 30, 'n_estimators': 500}\n",
      "-0.099446 (0.110214) with: {'criterion': 'mse', 'min_samples_leaf': 5, 'min_samples_split': 30, 'n_estimators': 1000}\n",
      "-0.093536 (0.113195) with: {'criterion': 'mse', 'min_samples_leaf': 5, 'min_samples_split': 35, 'n_estimators': 500}\n",
      "-0.088568 (0.107074) with: {'criterion': 'mse', 'min_samples_leaf': 5, 'min_samples_split': 35, 'n_estimators': 1000}\n",
      "-0.087985 (0.107643) with: {'criterion': 'mse', 'min_samples_leaf': 5, 'min_samples_split': 40, 'n_estimators': 500}\n",
      "-0.083499 (0.102970) with: {'criterion': 'mse', 'min_samples_leaf': 5, 'min_samples_split': 40, 'n_estimators': 1000}\n",
      "-0.122450 (0.112623) with: {'criterion': 'mse', 'min_samples_leaf': 7, 'min_samples_split': 20, 'n_estimators': 500}\n",
      "-0.120916 (0.108028) with: {'criterion': 'mse', 'min_samples_leaf': 7, 'min_samples_split': 20, 'n_estimators': 1000}\n",
      "-0.109886 (0.105634) with: {'criterion': 'mse', 'min_samples_leaf': 7, 'min_samples_split': 25, 'n_estimators': 500}\n",
      "-0.108855 (0.100561) with: {'criterion': 'mse', 'min_samples_leaf': 7, 'min_samples_split': 25, 'n_estimators': 1000}\n",
      "-0.082451 (0.105304) with: {'criterion': 'mse', 'min_samples_leaf': 7, 'min_samples_split': 30, 'n_estimators': 500}\n",
      "-0.081048 (0.099755) with: {'criterion': 'mse', 'min_samples_leaf': 7, 'min_samples_split': 30, 'n_estimators': 1000}\n",
      "-0.073018 (0.101989) with: {'criterion': 'mse', 'min_samples_leaf': 7, 'min_samples_split': 35, 'n_estimators': 500}\n",
      "-0.070986 (0.097355) with: {'criterion': 'mse', 'min_samples_leaf': 7, 'min_samples_split': 35, 'n_estimators': 1000}\n",
      "-0.068451 (0.098494) with: {'criterion': 'mse', 'min_samples_leaf': 7, 'min_samples_split': 40, 'n_estimators': 500}\n",
      "-0.067106 (0.094422) with: {'criterion': 'mse', 'min_samples_leaf': 7, 'min_samples_split': 40, 'n_estimators': 1000}\n",
      "-0.095397 (0.108242) with: {'criterion': 'mse', 'min_samples_leaf': 10, 'min_samples_split': 20, 'n_estimators': 500}\n",
      "-0.093714 (0.099664) with: {'criterion': 'mse', 'min_samples_leaf': 10, 'min_samples_split': 20, 'n_estimators': 1000}\n",
      "-0.091919 (0.104343) with: {'criterion': 'mse', 'min_samples_leaf': 10, 'min_samples_split': 25, 'n_estimators': 500}\n",
      "-0.090141 (0.094745) with: {'criterion': 'mse', 'min_samples_leaf': 10, 'min_samples_split': 25, 'n_estimators': 1000}\n",
      "-0.069122 (0.102852) with: {'criterion': 'mse', 'min_samples_leaf': 10, 'min_samples_split': 30, 'n_estimators': 500}\n",
      "-0.068260 (0.094200) with: {'criterion': 'mse', 'min_samples_leaf': 10, 'min_samples_split': 30, 'n_estimators': 1000}\n",
      "-0.061641 (0.100621) with: {'criterion': 'mse', 'min_samples_leaf': 10, 'min_samples_split': 35, 'n_estimators': 500}\n",
      "-0.061134 (0.093065) with: {'criterion': 'mse', 'min_samples_leaf': 10, 'min_samples_split': 35, 'n_estimators': 1000}\n",
      "-0.059243 (0.097944) with: {'criterion': 'mse', 'min_samples_leaf': 10, 'min_samples_split': 40, 'n_estimators': 500}\n",
      "-0.058383 (0.090962) with: {'criterion': 'mse', 'min_samples_leaf': 10, 'min_samples_split': 40, 'n_estimators': 1000}\n",
      "-0.198286 (0.139193) with: {'criterion': 'mae', 'min_samples_leaf': 1, 'min_samples_split': 20, 'n_estimators': 500}\n",
      "-0.195073 (0.135732) with: {'criterion': 'mae', 'min_samples_leaf': 1, 'min_samples_split': 20, 'n_estimators': 1000}\n",
      "-0.174842 (0.128735) with: {'criterion': 'mae', 'min_samples_leaf': 1, 'min_samples_split': 25, 'n_estimators': 500}\n",
      "-0.171805 (0.126212) with: {'criterion': 'mae', 'min_samples_leaf': 1, 'min_samples_split': 25, 'n_estimators': 1000}\n",
      "-0.127937 (0.128393) with: {'criterion': 'mae', 'min_samples_leaf': 1, 'min_samples_split': 30, 'n_estimators': 500}\n",
      "-0.123877 (0.126479) with: {'criterion': 'mae', 'min_samples_leaf': 1, 'min_samples_split': 30, 'n_estimators': 1000}\n",
      "-0.113521 (0.135046) with: {'criterion': 'mae', 'min_samples_leaf': 1, 'min_samples_split': 35, 'n_estimators': 500}\n",
      "-0.109185 (0.131142) with: {'criterion': 'mae', 'min_samples_leaf': 1, 'min_samples_split': 35, 'n_estimators': 1000}\n",
      "-0.103981 (0.131517) with: {'criterion': 'mae', 'min_samples_leaf': 1, 'min_samples_split': 40, 'n_estimators': 500}\n",
      "-0.100230 (0.130334) with: {'criterion': 'mae', 'min_samples_leaf': 1, 'min_samples_split': 40, 'n_estimators': 1000}\n",
      "-0.156700 (0.153744) with: {'criterion': 'mae', 'min_samples_leaf': 3, 'min_samples_split': 20, 'n_estimators': 500}\n",
      "-0.150835 (0.150570) with: {'criterion': 'mae', 'min_samples_leaf': 3, 'min_samples_split': 20, 'n_estimators': 1000}\n",
      "-0.141633 (0.142082) with: {'criterion': 'mae', 'min_samples_leaf': 3, 'min_samples_split': 25, 'n_estimators': 500}\n",
      "-0.137702 (0.141429) with: {'criterion': 'mae', 'min_samples_leaf': 3, 'min_samples_split': 25, 'n_estimators': 1000}\n",
      "-0.100384 (0.133806) with: {'criterion': 'mae', 'min_samples_leaf': 3, 'min_samples_split': 30, 'n_estimators': 500}\n",
      "-0.094983 (0.129845) with: {'criterion': 'mae', 'min_samples_leaf': 3, 'min_samples_split': 30, 'n_estimators': 1000}\n",
      "-0.089184 (0.134481) with: {'criterion': 'mae', 'min_samples_leaf': 3, 'min_samples_split': 35, 'n_estimators': 500}\n",
      "-0.085735 (0.130867) with: {'criterion': 'mae', 'min_samples_leaf': 3, 'min_samples_split': 35, 'n_estimators': 1000}\n",
      "-0.086646 (0.132875) with: {'criterion': 'mae', 'min_samples_leaf': 3, 'min_samples_split': 40, 'n_estimators': 500}\n",
      "-0.082862 (0.129615) with: {'criterion': 'mae', 'min_samples_leaf': 3, 'min_samples_split': 40, 'n_estimators': 1000}\n",
      "-0.154171 (0.167146) with: {'criterion': 'mae', 'min_samples_leaf': 5, 'min_samples_split': 20, 'n_estimators': 500}\n",
      "-0.145809 (0.159517) with: {'criterion': 'mae', 'min_samples_leaf': 5, 'min_samples_split': 20, 'n_estimators': 1000}\n",
      "-0.138117 (0.145678) with: {'criterion': 'mae', 'min_samples_leaf': 5, 'min_samples_split': 25, 'n_estimators': 500}\n",
      "-0.133631 (0.141872) with: {'criterion': 'mae', 'min_samples_leaf': 5, 'min_samples_split': 25, 'n_estimators': 1000}\n",
      "-0.090037 (0.128850) with: {'criterion': 'mae', 'min_samples_leaf': 5, 'min_samples_split': 30, 'n_estimators': 500}\n",
      "-0.088069 (0.127041) with: {'criterion': 'mae', 'min_samples_leaf': 5, 'min_samples_split': 30, 'n_estimators': 1000}\n",
      "-0.081498 (0.133362) with: {'criterion': 'mae', 'min_samples_leaf': 5, 'min_samples_split': 35, 'n_estimators': 500}\n",
      "-0.078405 (0.128850) with: {'criterion': 'mae', 'min_samples_leaf': 5, 'min_samples_split': 35, 'n_estimators': 1000}\n",
      "-0.077788 (0.130159) with: {'criterion': 'mae', 'min_samples_leaf': 5, 'min_samples_split': 40, 'n_estimators': 500}\n",
      "-0.075743 (0.125420) with: {'criterion': 'mae', 'min_samples_leaf': 5, 'min_samples_split': 40, 'n_estimators': 1000}\n",
      "-0.135577 (0.163043) with: {'criterion': 'mae', 'min_samples_leaf': 7, 'min_samples_split': 20, 'n_estimators': 500}\n",
      "-0.133844 (0.159920) with: {'criterion': 'mae', 'min_samples_leaf': 7, 'min_samples_split': 20, 'n_estimators': 1000}\n",
      "-0.123935 (0.148785) with: {'criterion': 'mae', 'min_samples_leaf': 7, 'min_samples_split': 25, 'n_estimators': 500}\n",
      "-0.122965 (0.145696) with: {'criterion': 'mae', 'min_samples_leaf': 7, 'min_samples_split': 25, 'n_estimators': 1000}\n",
      "-0.078247 (0.128588) with: {'criterion': 'mae', 'min_samples_leaf': 7, 'min_samples_split': 30, 'n_estimators': 500}\n",
      "-0.078998 (0.125842) with: {'criterion': 'mae', 'min_samples_leaf': 7, 'min_samples_split': 30, 'n_estimators': 1000}\n",
      "-0.068019 (0.126794) with: {'criterion': 'mae', 'min_samples_leaf': 7, 'min_samples_split': 35, 'n_estimators': 500}\n",
      "-0.068359 (0.124541) with: {'criterion': 'mae', 'min_samples_leaf': 7, 'min_samples_split': 35, 'n_estimators': 1000}\n",
      "-0.069462 (0.127370) with: {'criterion': 'mae', 'min_samples_leaf': 7, 'min_samples_split': 40, 'n_estimators': 500}\n",
      "-0.069255 (0.124954) with: {'criterion': 'mae', 'min_samples_leaf': 7, 'min_samples_split': 40, 'n_estimators': 1000}\n",
      "-0.105656 (0.138164) with: {'criterion': 'mae', 'min_samples_leaf': 10, 'min_samples_split': 20, 'n_estimators': 500}\n",
      "-0.103842 (0.137016) with: {'criterion': 'mae', 'min_samples_leaf': 10, 'min_samples_split': 20, 'n_estimators': 1000}\n",
      "-0.101505 (0.131908) with: {'criterion': 'mae', 'min_samples_leaf': 10, 'min_samples_split': 25, 'n_estimators': 500}\n",
      "-0.100300 (0.129501) with: {'criterion': 'mae', 'min_samples_leaf': 10, 'min_samples_split': 25, 'n_estimators': 1000}\n",
      "-0.070953 (0.121736) with: {'criterion': 'mae', 'min_samples_leaf': 10, 'min_samples_split': 30, 'n_estimators': 500}\n",
      "-0.071511 (0.121541) with: {'criterion': 'mae', 'min_samples_leaf': 10, 'min_samples_split': 30, 'n_estimators': 1000}\n",
      "-0.065231 (0.121526) with: {'criterion': 'mae', 'min_samples_leaf': 10, 'min_samples_split': 35, 'n_estimators': 500}\n",
      "-0.064791 (0.120102) with: {'criterion': 'mae', 'min_samples_leaf': 10, 'min_samples_split': 35, 'n_estimators': 1000}\n",
      "-0.065829 (0.121585) with: {'criterion': 'mae', 'min_samples_leaf': 10, 'min_samples_split': 40, 'n_estimators': 500}\n",
      "-0.065270 (0.120109) with: {'criterion': 'mae', 'min_samples_leaf': 10, 'min_samples_split': 40, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "# Grid Search for hyperparameters optimization\n",
    "criterion = ['mse', 'mae']\n",
    "n_estimators = [500, 1000]\n",
    "min_samples_split = [20, 25, 30, 35, 40]\n",
    "min_samples_leaf = [1, 3, 5, 7, 10]\n",
    "\n",
    "param_grid = dict(min_samples_split = min_samples_split, n_estimators = n_estimators, \n",
    "                  criterion = criterion, min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits = 5)\n",
    "grid_search = GridSearchCV(estimator = clf, param_grid = param_grid, scoring = 'r2', cv = tscv, n_jobs = -1)\n",
    "grid_search = grid_search.fit(X = X_train, y = y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_search.best_score_, grid_search.best_params_))\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.064354672908006"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the R Squared out of sample\n",
    "my_R_oos_sq(y_test, y_pred_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.83694301e-04,  1.16756612e-03, -1.17040101e-02,  5.26373607e-03,\n",
       "        4.86441558e-03, -3.96670457e-03,  2.84780091e-03, -6.65735748e-03,\n",
       "        5.97775513e-02,  2.97080357e-02,  1.72793915e-02,  2.50137011e-02,\n",
       "        6.99564640e-02,  7.10755690e-02,  3.47872834e-02,  3.99780218e-02,\n",
       "        2.83156698e-02,  2.22104506e-02,  8.38719716e-03,  6.03479782e-03,\n",
       "        6.38502240e-03,  6.53010048e-03,  2.00845832e-03,  3.86180998e-02,\n",
       "        3.87815503e-02,  3.37982702e-02,  4.56772926e-03,  4.79829316e-02,\n",
       "        1.44048593e-02,  3.66188044e-03,  2.45103722e-03,  1.82521529e-03,\n",
       "        3.73013140e-03,  3.26270924e-03, -3.24591073e-03,  4.08627492e-03,\n",
       "        6.47008694e-03,  6.10859611e-03, -6.51070637e-04,  1.16704789e-03,\n",
       "        1.35046906e-02, -7.15914036e-03, -4.28587934e-03,  4.69726883e-02,\n",
       "        2.48505528e-02,  2.56902612e-02,  2.88344257e-02,  1.35193436e-02,\n",
       "        1.19831382e-03,  3.43856670e-03, -6.95993230e-03,  6.05816561e-03,\n",
       "        5.74647348e-03,  8.83830414e-03,  1.40899822e-02,  1.47509529e-02,\n",
       "        2.32421668e-02,  1.33889848e-02,  2.84102875e-02,  1.37226218e-02,\n",
       "        1.79440246e-02,  1.04072753e-02,  1.19679229e-02,  8.74722299e-03,\n",
       "        1.34443120e-02, -3.17356174e-03,  9.41299677e-03,  7.55927360e-03,\n",
       "        1.56466850e-02,  1.75766995e-02,  1.54123530e-02, -5.05623464e-03,\n",
       "        1.32321750e-02,  1.41094484e-02,  9.17673333e-03,  1.19375638e-02,\n",
       "        1.79857456e-02,  5.00910264e-03,  1.85213780e-02,  1.87565278e-02,\n",
       "        2.50341313e-03,  3.96862049e-04, -2.83896925e-03, -3.63204112e-03,\n",
       "        6.72478681e-03, -1.37691705e-03,  1.52500294e-03,  1.71076931e-03,\n",
       "        4.96824397e-03, -2.56168177e-03,  6.49076622e-03,  5.40566266e-04,\n",
       "        1.41931118e-02,  5.96415704e-03,  9.10181501e-03,  5.58552272e-03,\n",
       "       -1.50227931e-03, -3.98273458e-05,  1.23537531e-02,  1.24790409e-02,\n",
       "        1.72409241e-02, -1.29657618e-03,  8.53876083e-03,  1.98480998e-02,\n",
       "        1.36324995e-02,  1.66564896e-02,  5.08048700e-03,  1.80706463e-02,\n",
       "        1.51140259e-02,  1.27540048e-02,  7.43174290e-03, -9.48528394e-04,\n",
       "        5.64036499e-04,  1.88754286e-02,  1.24840410e-02,  1.21923567e-02,\n",
       "        1.76604331e-02,  1.69086367e-02,  8.01161968e-03,  2.06027734e-02,\n",
       "        1.60787923e-02,  2.03902079e-02,  1.68803576e-03,  1.86436573e-02,\n",
       "        2.75395055e-03,  1.75103402e-02,  6.99376548e-03,  1.10876514e-02,\n",
       "        1.38559364e-02])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7667145864ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mPred_RF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mVal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mres_RF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Val' is not defined"
     ]
    }
   ],
   "source": [
    "# Output Binary Classification\n",
    "Pred_RF = []\n",
    "for i in range(len(y_pred_r)):\n",
    "    if y_pred_r[i] >= 0:\n",
    "        Pred_RF.append(1)\n",
    "    else:\n",
    "        Pred_RF.append(0)\n",
    "        \n",
    "count = 0\n",
    "for k in range(len(y_pred_r)):\n",
    "    if Pred_RF[k] == Val[k]:\n",
    "        count += 1\n",
    "res_RF = count/len(y_pred)\n",
    "res_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cla = XGBRegressor(max_depth = 4, learning_rate = 0.1, n_estimators = 100, verbosity = 1,\n",
    "    silent = None, booster = 'gbtree', objective = 'reg:squarederror', n_jobs = 1, nthread = None, \n",
    "    gamma = 0, min_child_weight = 1, max_delta_step = 0, subsample = 1, colsample_bytree = 1, \n",
    "    colsample_bylevel = 1, colsample_bynode = 1, reg_alpha = 0, reg_lambda = 1, scale_pos_weight = 1, \n",
    "    base_score = 0.5, random_state= 69, seed = None, missing = None, importance_type = 'gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cla.fit(X_oostrain, y_oostrain)\n",
    "y_pred_boost = cla.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12708786512701287"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the R Squared out of sample\n",
    "my_R_oos_sq(y_test, y_pred_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5503875968992248"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output Binary Classification\n",
    "Pred_XGB = []\n",
    "for i in range(len(y_pred_boost)):\n",
    "    if y_pred_boost[i] >= 0:\n",
    "        Pred_XGB.append(1)\n",
    "    else:\n",
    "        Pred_XGB.append(0)\n",
    "        \n",
    "count = 0\n",
    "for k in range(len(y_pred)):\n",
    "    if Pred_XGB[k] == Val[k]:\n",
    "        count += 1\n",
    "res_XGB = count/len(y_pred)\n",
    "res_XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(a, b, c):\n",
    "    Pred_ENS = []\n",
    "    for i in range(len(a)):\n",
    "        if a[i] + b[i] + c[i] == 3:\n",
    "            Pred_ENS.append(1)\n",
    "        elif a[i] + b[i] + c[i] == 2:\n",
    "            Pred_ENS.append(1)\n",
    "        elif a[i] + b[i] + c[i] == 1:\n",
    "            Pred_ENS.append(0)\n",
    "        elif a[i] + b[i] + c[i] == 0:\n",
    "            Pred_ENS.append(0)\n",
    "    return Pred_ENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5736434108527132"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pred_ENS = ensemble(Pred_NN, Pred_RF, Pred_XGB)\n",
    "\n",
    "count = 0\n",
    "for k in range(len(y_pred)):\n",
    "    if Pred_ENS[k] == Val[k]:\n",
    "        count += 1\n",
    "res_ENS = count/len(y_pred)\n",
    "res_ENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
